{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f51c4e46-a2d6-450a-b7b5-61e914b504f7",
   "metadata": {},
   "source": [
    "# Create Input X for just one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff2f9da8-562e-485c-a1fe-f51ef8d03c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc4095df-50ac-46f4-b035-8d7934acc90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awkward version: 1.4.0\n",
      "Uproot version: 4.0.11\n"
     ]
    }
   ],
   "source": [
    "#====================\n",
    "# Load Utils ========\n",
    "#====================\n",
    "\n",
    "import numpy as np\n",
    "import uproot as ur\n",
    "import awkward as ak\n",
    "import time as t\n",
    "import os\n",
    "print(\"Awkward version: \"+str(ak.__version__))\n",
    "print(\"Uproot version: \"+str(ur.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf25d215-e0be-4949-ba6b-8970780434c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================\n",
    "# Metadata ==========\n",
    "#====================\n",
    "track_branches = ['trackEta_EMB1', 'trackPhi_EMB1', 'trackEta_EMB2', 'trackPhi_EMB2', 'trackEta_EMB3', 'trackPhi_EMB3',\n",
    "                  'trackEta_TileBar0', 'trackPhi_TileBar0', 'trackEta_TileBar1', 'trackPhi_TileBar1',\n",
    "                  'trackEta_TileBar2', 'trackPhi_TileBar2']\n",
    "\n",
    "event_branches = [\"cluster_nCells\", \"cluster_cell_ID\", \"cluster_cell_E\", 'cluster_nCells', \"nCluster\", \"eventNumber\",\n",
    "                  \"nTrack\", \"nTruthPart\", \"truthPartPdgId\", \"cluster_Eta\", \"cluster_Phi\", 'trackPt', 'trackP',\n",
    "                  'trackMass', 'trackEta', 'trackPhi', 'truthPartE', 'cluster_ENG_CALIB_TOT', \"cluster_E\", 'truthPartPt']\n",
    "\n",
    "ak_event_branches = [\"cluster_nCells\", \"cluster_cell_ID\", \"cluster_cell_E\", \"cluster_nCells\",\n",
    "                  \"nTruthPart\", \"truthPartPdgId\", \"cluster_Eta\", \"cluster_Phi\", \"trackPt\", \"trackP\",\n",
    "                  \"trackMass\", \"trackEta\", \"trackPhi\", \"truthPartE\", \"cluster_ENG_CALIB_TOT\", \"cluster_E\", \"truthPartPt\"]\n",
    "\n",
    "np_event_branches = [\"nCluster\", \"eventNumber\", \"nTrack\", \"nTruthPart\"]\n",
    "\n",
    "geo_branches = [\"cell_geo_ID\", \"cell_geo_eta\", \"cell_geo_phi\", \"cell_geo_rPerp\", \"cell_geo_sampling\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5be4154-cd15-4275-846f-f9c6c4305999",
   "metadata": {},
   "source": [
    "# The functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d45a4c3-6430-42bc-8931-cac4afb67517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_from_tree(tree, branches=None, np_branches=None):\n",
    "    ''' Loads branches as default awkward arrays and np_branches as numpy arrays. '''\n",
    "    dictionary = dict()\n",
    "    if branches is not None:\n",
    "        for key in branches:\n",
    "            branch = tree.arrays()[key]\n",
    "            dictionary[key] = branch\n",
    "            \n",
    "    if np_branches is not None:\n",
    "        for np_key in np_branches:\n",
    "            np_branch = np.ndarray.flatten(tree.arrays()[np_key].to_numpy())\n",
    "            dictionary[np_key] = np_branch\n",
    "    \n",
    "    if branches is None and np_branches is None:\n",
    "        raise ValueError(\"No branches passed to function.\")\n",
    "        \n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe81f659-24ea-4dbc-ba00-4d5afedaae69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_dim_tuple(events, event_dict):\n",
    "    nEvents = len(events)\n",
    "    max_clust = 0\n",
    "    \n",
    "    for i in range(nEvents):\n",
    "        event = events[i,0]\n",
    "        track_nums = events[i,1]\n",
    "        clust_nums = events[i,2]\n",
    "        \n",
    "        clust_num_total = 0\n",
    "        # set this to six for now to handle single track events, change later\n",
    "        track_num_total = 6\n",
    "        \n",
    "        # Check if there are clusters, None type object may be associated with it\n",
    "        if clust_nums is not None:\n",
    "            # Search through cluster indices\n",
    "            for clst_idx in clust_nums:\n",
    "                nInClust = len(event_dict['cluster_cell_ID'][event][clst_idx])\n",
    "                # add the number in each cluster to the total\n",
    "                clust_num_total += nInClust\n",
    "\n",
    "        total_size = clust_num_total + track_num_total\n",
    "        if total_size > max_clust:\n",
    "            max_clust = total_size\n",
    "    \n",
    "    # 6 for energy, eta, phi, rperp, track flag, sample layer\n",
    "    return (nEvents, max_clust, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8c72a88-0a46-423e-8bd9-e30cde0ff490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_index_1D(values, dictionary):\n",
    "    ''' Use a for loop and a dictionary. values are the IDs to search for. dict must be in format \n",
    "    (cell IDs: index) '''\n",
    "    idx_vec = np.zeros(len(values), dtype=np.int32)\n",
    "    for i in range(len(values)):\n",
    "        idx_vec[i] = dictionary[values[i]]\n",
    "    return idx_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07258471-9b79-4e74-a70b-e748b74f65ee",
   "metadata": {},
   "source": [
    "# The data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0aff76fa-88db-454a-b5a8-c55c9f839909",
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================\n",
    "# File setup ========\n",
    "#====================\n",
    "# user.angerami.24559744.OutputStream._000001.root\n",
    "# Number of files\n",
    "Nfile = 1\n",
    "fileNames = []\n",
    "file_prefix = 'user.angerami.24559744.OutputStream._000'\n",
    "for i in range(1,Nfile+1):\n",
    "    endstring = f'{i:03}'\n",
    "    fileNames.append(file_prefix + endstring + '.root')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ab10778-1174-4fe7-9ee0-cebdb37577b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================\n",
    "# Load Data Files ===\n",
    "#====================\n",
    "\n",
    "## GEOMETRY DICTIONARY ##\n",
    "geo_file = ur.open('/fast_scratch/atlas_images/v01-45/cell_geo.root')\n",
    "CellGeo_tree = geo_file[\"CellGeo\"]\n",
    "geo_dict = dict_from_tree(tree=CellGeo_tree, branches=None, np_branches=geo_branches)\n",
    "\n",
    "# cell geometry data\n",
    "cell_geo_ID = geo_dict['cell_geo_ID']\n",
    "cell_ID_dict = dict(zip(cell_geo_ID, np.arange(len(cell_geo_ID))))\n",
    "\n",
    "# for event dictionary\n",
    "events_prefix = '/fast_scratch/atlas_images/v01-45/pipm/'\n",
    "\n",
    "# Use this to compare with the dimensionality of new events\n",
    "firstArray = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb741fb-06f1-482e-992d-f93e4cee9a31",
   "metadata": {},
   "source": [
    "# Loop over files: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac829811-23a3-4167-96db-aba9b594f8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1 # tally used to keep track of file number\n",
    "tot_nEvts = 0 # used for keeping track of total number of events\n",
    "max_nPoints = 0 # used for keeping track of the largest 'point cloud'\n",
    "t_tot = 0 # total time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aaa3711e-8ee9-423b-958d-893aeb6bdc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for currFile in fileNames:\n",
    "#    \n",
    "#    # Check for file, a few are missing\n",
    "#    if not os.path.isfile(events_prefix+currFile):\n",
    "#        print()\n",
    "#        print('File '+events_prefix+currFile+' not found..')\n",
    "#        print()\n",
    "##        k += 1\n",
    "#        continue\n",
    "#    \n",
    "#    else:\n",
    "#        print()\n",
    "#        print('Working on File: '+str(currFile)+' - '+str(k)+'/'+str(Nfile))\n",
    "#        k += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff3fcb3-3399-44d8-aab8-59d2031ab78d",
   "metadata": {},
   "source": [
    "Just test one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f83d3c17-fa92-4448-8013-2951028472e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "currFile = 'user.angerami.24559744.OutputStream._000001.root'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1204720a-4ed7-490d-b68a-5403ebb4d4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## EVENT DICTIONARY ##\n",
    "event = ur.open(events_prefix+currFile)\n",
    "event_tree = event[\"EventTree\"]\n",
    "event_dict = dict_from_tree(tree=event_tree, branches=ak_event_branches, np_branches=np_event_branches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eb7507-16b4-4126-b615-052095bdd732",
   "metadata": {},
   "source": [
    "ak_event_branches -> Cluster, tracks, etcs. arrays per event\n",
    "\n",
    "np_branches -> Event-level variables. one value per event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d3123a67-3fce-4373-9d34-667aaa85fd21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cluster_nCells',\n",
       " 'cluster_cell_ID',\n",
       " 'cluster_cell_E',\n",
       " 'cluster_nCells',\n",
       " 'nTruthPart',\n",
       " 'truthPartPdgId',\n",
       " 'cluster_Eta',\n",
       " 'cluster_Phi',\n",
       " 'trackPt',\n",
       " 'trackP',\n",
       " 'trackMass',\n",
       " 'trackEta',\n",
       " 'trackPhi',\n",
       " 'truthPartE',\n",
       " 'cluster_ENG_CALIB_TOT',\n",
       " 'cluster_E',\n",
       " 'truthPartPt']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ak_event_branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "26a70766-6d8b-4be1-9dd9-108345b65ccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nCluster', 'eventNumber', 'nTrack', 'nTruthPart']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_event_branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cce603f3-9059-4e1b-82ee-900f11b4f8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Events with 0 clusters: 6597\n",
      "Total number of events:  20000\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "elements_count = collections.Counter(event_dict['nCluster'])\n",
    "for key, value in elements_count.items():\n",
    "    if key == 0:\n",
    "       print(f\"Events with {key} clusters: {value}\")\n",
    "print(\"Total number of events: \", len(event_dict['nCluster']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bcdbe7-2d4b-4a4b-a364-64a1bb3be353",
   "metadata": {},
   "source": [
    "# Event level cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c185cadb-73d8-4e1d-8d0c-999348cf4fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================\n",
    "# APPLY CUTS =======\n",
    "#===================\n",
    "# create ordered list of events to use for index slicing\n",
    "nEvents = len(event_dict['eventNumber'])\n",
    "all_events = np.arange(0,nEvents,1,dtype=np.int32) #array with event index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2a163f11-3034-40a9-b900-620fa3d17c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nCluster = event_dict['nCluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "18962c0c-1b71-4eab-bd0d-173718d99512",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_event_mask = nCluster != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5349966d-a151-4996-87bc-8ba2ef861cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_event = all_events[filtered_event_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "73ccceb0-76e5-4886-b091-37249245bd0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* selected events:  13403 / 20000\n"
     ]
    }
   ],
   "source": [
    "print(\"* selected events: \", len(filtered_event), \"/\", len(all_events))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58e011f-a3cc-419e-a320-fc8917a42b89",
   "metadata": {},
   "source": [
    "First event does not have clusters and was filtered out:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12f9584-0239-4522-b12a-3447771a20a2",
   "metadata": {},
   "source": [
    "Second event have one cluster with 105 cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f37665-8f25-4871-8a64-d6a6d062801b",
   "metadata": {},
   "source": [
    "# Loop over events to create an index array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0f2ca60d-0cde-44dd-acb5-35945a2bf9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================================#\n",
    "## CREATE INDEX ARRAY FOR  CLUSTERS ##\n",
    "#============================================#\n",
    "event_indices = []\n",
    "t0 = t.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4421d837-6483-4a0e-81d1-b6a29c24e948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Event  3000  has  1 / 2  selected clusters\n",
      "** Event  5000  has  3 / 4  selected clusters\n",
      "** Event  11000  has  3 / 3  selected clusters\n",
      "** Event  13000  has  1 / 1  selected clusters\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    for evt in filtered_event:\n",
    "        # pull cluster number, don't need zero index as it's loaded as a np array\n",
    "        nClust = event_dict[\"nCluster\"][evt]\n",
    "        cluster_idx = np.arange(nClust)\n",
    "\n",
    "        ## Cluster properties\n",
    "        clusterEs = event_dict[\"cluster_E\"][evt].to_numpy()\n",
    "        clus_phi = event_dict[\"cluster_Phi\"][evt].to_numpy()\n",
    "        clus_eta = event_dict[\"cluster_Eta\"][evt].to_numpy()\n",
    "\n",
    "        ## ENERGY SELECTION AND ETA SELECTION\n",
    "        eta_mask = abs(clus_eta) < 0.7\n",
    "        e_mask = clusterEs > 0.5\n",
    "        selection = eta_mask & e_mask\n",
    "        selected_clusters = cluster_idx[selection]\n",
    "\n",
    "        ## CREATE LIST ##\n",
    "        if np.count_nonzero(selection) > 0:\n",
    "            event_indices.append((evt, 0, selected_clusters))\n",
    "            if (evt % 1000==0):\n",
    "                print(\"** Event \", evt, \" has \", np.count_nonzero(selection), \"/\" ,nClust,\" selected clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "710ab897-3149-4a64-b467-f3347ecc67ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_indices = np.array(event_indices, dtype=np.object_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3af606-bcee-474b-88bd-8c812d58f440",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "672d1147-a029-4168-93c9-1a6d6ccfcb15",
   "metadata": {},
   "source": [
    "# Max dimension of X array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5ddb282d-93a3-4733-9d6a-5f6a1bf3a9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Events with selected clusters: 3722\n",
      "* Total number of cells: 942\n",
      "* Dim of largest point cloud: 942\n"
     ]
    }
   ],
   "source": [
    "    #=========================#\n",
    "    ## DIMENSIONS OF X ARRAY ##\n",
    "    #=========================#\n",
    "    max_dims = find_max_dim_tuple(event_indices, event_dict)\n",
    "    evt_tot = max_dims[0]\n",
    "    tot_nEvts += max_dims[0]\n",
    "    # keep track of the largest point cloud to use for saving later\n",
    "    if max_dims[1] > max_nPoints:\n",
    "        max_nPoints = max_dims[1]\n",
    "\n",
    "    print('* Events with selected clusters: '+str(evt_tot))\n",
    "    print('* Total number of cells: '+str(max_dims[1]))\n",
    "    print('* Dim of largest point cloud: '+str(max_nPoints))\n",
    "\n",
    "    # Create arrays\n",
    "    Y_new = np.zeros((max_dims[0],3))\n",
    "    X_new = np.zeros(max_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1823431-39f8-4ab7-82e6-6c6db4171bdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e617af4-cd58-4a1a-8e6b-f59e64ddcd15",
   "metadata": {},
   "source": [
    "# Fill the entires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d035c647-b45c-4394-b1a0-eda9dddc49d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(max_dims[0]):\n",
    "        # pull all relevant indices\n",
    "        evt = event_indices[i,0]\n",
    "        track_idx = event_indices[i,1]\n",
    "        # recall this now returns an array\n",
    "        cluster_nums = event_indices[i,2]\n",
    "\n",
    "        ##############\n",
    "        ## CLUSTERS ##\n",
    "        ##############\n",
    "        # set up to have no clusters, further this with setting up the same thing for tracks\n",
    "        target_ENG_CALIB_TOT = -1\n",
    "        if cluster_nums is not None:\n",
    "\n",
    "            # find averaged center of clusters\n",
    "            cluster_Eta = event_dict['cluster_Eta'][evt].to_numpy()\n",
    "            cluster_Phi = event_dict['cluster_Phi'][evt].to_numpy()\n",
    "            av_Eta = np.mean(cluster_Eta)\n",
    "            av_Phi = np.mean(cluster_Phi)\n",
    "\n",
    "            nClust_current_total = 0\n",
    "            target_ENG_CALIB_TOT = 0\n",
    "\n",
    "            for c in cluster_nums:            \n",
    "                # cluster data\n",
    "                target_ENG_CALIB_TOT += event_dict['cluster_ENG_CALIB_TOT'][evt][c]\n",
    "                cluster_cell_ID = event_dict['cluster_cell_ID'][evt][c].to_numpy()\n",
    "                nInClust = len(cluster_cell_ID)\n",
    "                cluster_cell_E = event_dict['cluster_cell_E'][evt][c].to_numpy()            \n",
    "                cell_indices = find_index_1D(cluster_cell_ID, cell_ID_dict)\n",
    "\n",
    "                cluster_cell_Eta = geo_dict['cell_geo_eta'][cell_indices]\n",
    "                cluster_cell_Phi = geo_dict['cell_geo_phi'][cell_indices]\n",
    "                cluster_cell_rPerp = geo_dict['cell_geo_rPerp'][cell_indices]\n",
    "                cluster_cell_sampling = geo_dict['cell_geo_sampling'][cell_indices]\n",
    "\n",
    "                # input all the data\n",
    "                # note here we leave the fourth entry zeros (zero for flag!!!)\n",
    "                low = nClust_current_total\n",
    "                high = low + nInClust\n",
    "                X_new[i,low:high,0] = np.log(cluster_cell_E)\n",
    "                # Normalize to average cluster centers\n",
    "                X_new[i,low:high,1] = cluster_cell_Eta - av_Eta #cluster_cell_Eta - event_dict['cluster_Eta'][evt][c]\n",
    "                X_new[i,low:high,2] = cluster_cell_Phi - av_Phi #cluster_cell_Phi -event_dict['cluster_Phi'][evt][c]\n",
    "                X_new[i,low:high,3] = cluster_cell_rPerp\n",
    "                X_new[i,low:high,5] = cluster_cell_sampling * 0.1\n",
    "\n",
    "                nClust_current_total += nInClust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d4c3bce4-46ab-4b18-a42b-9e5049b00805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3722, 942, 6)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7db117f0-e92a-4c02-baec-bd6757b3f1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "## TARGET ENERGIES ##\n",
    "#####################\n",
    "# this is needed for energy regression\n",
    "Y_new[i,0] = event_dict['truthPartE'][evt][0]\n",
    "Y_new[i,1] = event_dict['truthPartPt'][evt][track_idx]\n",
    "Y_new[i,2] = target_ENG_CALIB_TOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7415f68f-1991-48f3-a7d3-66592eb9f304",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffc3259-4cf8-42de-a373-cec82f804b3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c232a2e-c7a3-4795-bb84-c6a036c9dbe2",
   "metadata": {},
   "source": [
    "# Playing with records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8098b535-f09e-4fe0-9dbf-c5b7df371026",
   "metadata": {},
   "source": [
    "https://medium.com/mostly-ai/tensorflow-records-what-they-are-and-how-to-use-them-c46bc4bbb564"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d484648-3c54-482b-a0b7-1686dc61b1d6",
   "metadata": {},
   "source": [
    "tf.train.Example\n",
    "\n",
    "If your dataset consist of features, where each feature is a list of values of the same type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8978e657-76f1-4d00-8f93-97ea929d75d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-26 08:34:03.065275: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f76941b-a624-4c60-88ba-59ec6d4345e5",
   "metadata": {},
   "source": [
    "## Features\n",
    "\n",
    "Order in X_new[:,:,i]: i= 0:energy, 1:eta, 2:phi, 3:rperp, 4:sampleID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1c7292-db93-466b-8deb-63ec222fa372",
   "metadata": {
    "tags": []
   },
   "source": [
    "- *tf.train.Feature* wraps a list of data of a specific type so Tensorflow can understand it. \n",
    "\n",
    "\n",
    "single attribute: union of bytes_list/float_list/int64_list.\n",
    "the stored list can be of type tf.train.BytesList, tf.train.FloatList, or tf.train.Int64List "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "58762e94-aa77-4e50-98a2-bba4e6006c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _floats_feature(value):\n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7463fbf-59c0-4f89-a429-e1c05c0b831d",
   "metadata": {},
   "source": [
    "Features: Energy, $\\eta$, $\\phi$, sampleID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "009db457-8d6f-48d2-a3a6-8b8b1b125f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "ene_arr_flat= ak.flatten(X_new[:,:,0].astype(np.float64))\n",
    "eta_arr_flat= ak.flatten(X_new[:,:,1].astype(np.float64))\n",
    "phi_arr_flat= ak.flatten(X_new[:,:,2].astype(np.float64))\n",
    "sam_arr_flat= ak.flatten(X_new[:,:,4].astype(np.float64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530848f2-1edf-44bd-846e-2ba5498087d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "- *tf.train.Features* is a collection of named features.\n",
    "\n",
    "Single attribute: feature that expects a dictionary where the key is the name of the features and the value a tf.train.Feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "821ab41e-9878-495a-86dd-a3fbf0ea1b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dic = {\n",
    "    \"ene\": _floats_feature(ene_arr_flat),\n",
    "    \"eta\": _floats_feature(eta_arr_flat),\n",
    "    \"phi\": _floats_feature(phi_arr_flat),\n",
    "    \"sam\": _floats_feature(sam_arr_flat),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b97b8247-20ff-4d52-bee9-12743855a67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classif_features = tf.train.Features(feature=features_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2fc01b-7e92-4441-9382-e69c44913037",
   "metadata": {
    "tags": []
   },
   "source": [
    "- *tf.train.Example* stores features in a single attribute features of type tf.train.Features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ed902ae7-d137-489e-9bdd-f24685ba5950",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = tf.train.Example(features=classif_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "309b7bd4-3a60-4cbf-bfd0-e407ae1c79b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Just for eta\n",
    "example_eta = tf.train.Example(features=tf.train.Features(feature={\"eta\": _floats_feature(eta_arr_flat)}))\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6022f25a-37c2-49b5-a2c6-389dcbcf8e00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "933e2e8e-f1ca-40f5-b862-bf0e3d52c418",
   "metadata": {
    "tags": []
   },
   "source": [
    "- *tf.python_io.TFRecordWriter* to write\n",
    "\n",
    "Serialize and store the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a5ec39-8a8c-41d1-8d1e-4939d04a6a74",
   "metadata": {},
   "source": [
    "For tf2: tf.compat.v1.python_io.TFRecordWriter instead of tf.python_io.TFRecordWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "09fb0aae-de63-40eb-a1af-e97039dec790",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.compat.v1.python_io.TFRecordWriter('/data/atlas/dportill/X_1file.tfrecord') as writer:\n",
    "  writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d29374-371c-42b8-9209-5dac5c83af15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256dfaee-b2b5-403d-93cf-2a550f0f3dde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c9f1f2-b568-42fd-9df2-1f7c43f2dbff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdc3ae2-27a8-4dca-a65f-c790cbb10298",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
