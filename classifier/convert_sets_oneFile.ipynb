{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f51c4e46-a2d6-450a-b7b5-61e914b504f7",
   "metadata": {},
   "source": [
    "# Create TFRecord Input X for just one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff2f9da8-562e-485c-a1fe-f51ef8d03c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc4095df-50ac-46f4-b035-8d7934acc90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awkward version: 1.4.0\n",
      "Uproot version: 4.0.11\n"
     ]
    }
   ],
   "source": [
    "#====================\n",
    "# Load Utils ========\n",
    "#====================\n",
    "\n",
    "import numpy as np\n",
    "import uproot as ur\n",
    "import awkward as ak\n",
    "import time as t\n",
    "import os\n",
    "print(\"Awkward version: \"+str(ak.__version__))\n",
    "print(\"Uproot version: \"+str(ur.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf25d215-e0be-4949-ba6b-8970780434c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================\n",
    "# Metadata ==========\n",
    "#====================\n",
    "track_branches = ['trackEta_EMB1', 'trackPhi_EMB1', 'trackEta_EMB2', 'trackPhi_EMB2', 'trackEta_EMB3', 'trackPhi_EMB3',\n",
    "                  'trackEta_TileBar0', 'trackPhi_TileBar0', 'trackEta_TileBar1', 'trackPhi_TileBar1',\n",
    "                  'trackEta_TileBar2', 'trackPhi_TileBar2']\n",
    "\n",
    "event_branches = [\"cluster_nCells\", \"cluster_cell_ID\", \"cluster_cell_E\", 'cluster_nCells', \"nCluster\", \"eventNumber\",\n",
    "                  \"nTrack\", \"nTruthPart\", \"truthPartPdgId\", \"cluster_Eta\", \"cluster_Phi\", 'trackPt', 'trackP',\n",
    "                  'trackMass', 'trackEta', 'trackPhi', 'truthPartE', 'cluster_ENG_CALIB_TOT', \"cluster_E\", 'truthPartPt']\n",
    "\n",
    "ak_event_branches = [\"cluster_nCells\", \"cluster_cell_ID\", \"cluster_cell_E\", \"cluster_nCells\",\n",
    "                  \"nTruthPart\", \"truthPartPdgId\", \"cluster_Eta\", \"cluster_Phi\", \"trackPt\", \"trackP\",\n",
    "                  \"trackMass\", \"trackEta\", \"trackPhi\", \"truthPartE\", \"cluster_ENG_CALIB_TOT\", \"cluster_E\", \"truthPartPt\"]\n",
    "\n",
    "np_event_branches = [\"nCluster\", \"eventNumber\", \"nTrack\", \"nTruthPart\"]\n",
    "\n",
    "geo_branches = [\"cell_geo_ID\", \"cell_geo_eta\", \"cell_geo_phi\", \"cell_geo_rPerp\", \"cell_geo_sampling\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5be4154-cd15-4275-846f-f9c6c4305999",
   "metadata": {},
   "source": [
    "# The functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d45a4c3-6430-42bc-8931-cac4afb67517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_from_tree(tree, branches=None, np_branches=None):\n",
    "    ''' Loads branches as default awkward arrays and np_branches as numpy arrays. '''\n",
    "    dictionary = dict()\n",
    "    if branches is not None:\n",
    "        for key in branches:\n",
    "            branch = tree.arrays()[key]\n",
    "            dictionary[key] = branch\n",
    "            \n",
    "    if np_branches is not None:\n",
    "        for np_key in np_branches:\n",
    "            np_branch = np.ndarray.flatten(tree.arrays()[np_key].to_numpy())\n",
    "            dictionary[np_key] = np_branch\n",
    "    \n",
    "    if branches is None and np_branches is None:\n",
    "        raise ValueError(\"No branches passed to function.\")\n",
    "        \n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe81f659-24ea-4dbc-ba00-4d5afedaae69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_dim_tuple(events, event_dict):\n",
    "    nEvents = len(events)\n",
    "    max_clust = 0\n",
    "    \n",
    "    for i in range(nEvents):\n",
    "        event = events[i,0]\n",
    "        track_nums = events[i,1]\n",
    "        clust_nums = events[i,2]\n",
    "        \n",
    "        clust_num_total = 0\n",
    "        # set this to six for now to handle single track events, change later\n",
    "        track_num_total = 6\n",
    "        \n",
    "        # Check if there are clusters, None type object may be associated with it\n",
    "        if clust_nums is not None:\n",
    "            # Search through cluster indices\n",
    "            for clst_idx in clust_nums:\n",
    "                nInClust = len(event_dict['cluster_cell_ID'][event][clst_idx])\n",
    "                # add the number in each cluster to the total\n",
    "                clust_num_total += nInClust\n",
    "\n",
    "        total_size = clust_num_total + track_num_total\n",
    "        if total_size > max_clust:\n",
    "            max_clust = total_size\n",
    "    \n",
    "    # 6 for energy, eta, phi, rperp, track flag, sample layer\n",
    "    return (nEvents, max_clust, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8c72a88-0a46-423e-8bd9-e30cde0ff490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_index_1D(values, dictionary):\n",
    "    ''' Use a for loop and a dictionary. values are the IDs to search for. dict must be in format \n",
    "    (cell IDs: index) '''\n",
    "    idx_vec = np.zeros(len(values), dtype=np.int32)\n",
    "    for i in range(len(values)):\n",
    "        idx_vec[i] = dictionary[values[i]]\n",
    "    return idx_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07258471-9b79-4e74-a70b-e748b74f65ee",
   "metadata": {},
   "source": [
    "# The data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0aff76fa-88db-454a-b5a8-c55c9f839909",
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================\n",
    "# File setup ========\n",
    "#====================\n",
    "# user.angerami.24559744.OutputStream._000001.root\n",
    "# Number of files\n",
    "Nfile = 1\n",
    "fileNames = []\n",
    "file_prefix = 'user.angerami.24559744.OutputStream._000'\n",
    "for i in range(1,Nfile+1):\n",
    "    endstring = f'{i:03}'\n",
    "    fileNames.append(file_prefix + endstring + '.root')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ab10778-1174-4fe7-9ee0-cebdb37577b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================\n",
    "# Load Data Files ===\n",
    "#====================\n",
    "\n",
    "## GEOMETRY DICTIONARY ##\n",
    "geo_file = ur.open('/fast_scratch/atlas_images/v01-45/cell_geo.root')\n",
    "CellGeo_tree = geo_file[\"CellGeo\"]\n",
    "geo_dict = dict_from_tree(tree=CellGeo_tree, branches=None, np_branches=geo_branches)\n",
    "\n",
    "# cell geometry data\n",
    "cell_geo_ID = geo_dict['cell_geo_ID']\n",
    "cell_ID_dict = dict(zip(cell_geo_ID, np.arange(len(cell_geo_ID))))\n",
    "\n",
    "# for event dictionary\n",
    "events_prefix = '/fast_scratch/atlas_images/v01-45/pipm/'\n",
    "\n",
    "# Use this to compare with the dimensionality of new events\n",
    "firstArray = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb741fb-06f1-482e-992d-f93e4cee9a31",
   "metadata": {},
   "source": [
    "# Loop over files: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac829811-23a3-4167-96db-aba9b594f8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1 # tally used to keep track of file number\n",
    "tot_nEvts = 0 # used for keeping track of total number of events\n",
    "max_nPoints = 0 # used for keeping track of the largest 'point cloud'\n",
    "t_tot = 0 # total time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aaa3711e-8ee9-423b-958d-893aeb6bdc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for currFile in fileNames:\n",
    "#    \n",
    "#    # Check for file, a few are missing\n",
    "#    if not os.path.isfile(events_prefix+currFile):\n",
    "#        print()\n",
    "#        print('File '+events_prefix+currFile+' not found..')\n",
    "#        print()\n",
    "##        k += 1\n",
    "#        continue\n",
    "#    \n",
    "#    else:\n",
    "#        print()\n",
    "#        print('Working on File: '+str(currFile)+' - '+str(k)+'/'+str(Nfile))\n",
    "#        k += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff3fcb3-3399-44d8-aab8-59d2031ab78d",
   "metadata": {},
   "source": [
    "Just test one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f83d3c17-fa92-4448-8013-2951028472e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "currFile = 'user.angerami.24559744.OutputStream._000001.root'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1204720a-4ed7-490d-b68a-5403ebb4d4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## EVENT DICTIONARY ##\n",
    "event = ur.open(events_prefix+currFile)\n",
    "event_tree = event[\"EventTree\"]\n",
    "event_dict = dict_from_tree(tree=event_tree, branches=ak_event_branches, np_branches=np_event_branches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eb7507-16b4-4126-b615-052095bdd732",
   "metadata": {},
   "source": [
    "ak_event_branches -> Cluster, tracks, etcs. arrays per event\n",
    "\n",
    "np_branches -> Event-level variables. one value per event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d3123a67-3fce-4373-9d34-667aaa85fd21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cluster_nCells',\n",
       " 'cluster_cell_ID',\n",
       " 'cluster_cell_E',\n",
       " 'cluster_nCells',\n",
       " 'nTruthPart',\n",
       " 'truthPartPdgId',\n",
       " 'cluster_Eta',\n",
       " 'cluster_Phi',\n",
       " 'trackPt',\n",
       " 'trackP',\n",
       " 'trackMass',\n",
       " 'trackEta',\n",
       " 'trackPhi',\n",
       " 'truthPartE',\n",
       " 'cluster_ENG_CALIB_TOT',\n",
       " 'cluster_E',\n",
       " 'truthPartPt']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ak_event_branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "26a70766-6d8b-4be1-9dd9-108345b65ccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nCluster', 'eventNumber', 'nTrack', 'nTruthPart']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_event_branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cce603f3-9059-4e1b-82ee-900f11b4f8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Events with 0 clusters: 6597\n",
      "Total number of events:  20000\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "elements_count = collections.Counter(event_dict['nCluster'])\n",
    "for key, value in elements_count.items():\n",
    "    if key == 0:\n",
    "       print(f\"Events with {key} clusters: {value}\")\n",
    "print(\"Total number of events: \", len(event_dict['nCluster']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bcdbe7-2d4b-4a4b-a364-64a1bb3be353",
   "metadata": {},
   "source": [
    "# Event level cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c185cadb-73d8-4e1d-8d0c-999348cf4fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================\n",
    "# APPLY CUTS =======\n",
    "#===================\n",
    "# create ordered list of events to use for index slicing\n",
    "nEvents = len(event_dict['eventNumber'])\n",
    "all_events = np.arange(0,nEvents,1,dtype=np.int32) #array with event index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2a163f11-3034-40a9-b900-620fa3d17c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nCluster = event_dict['nCluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "18962c0c-1b71-4eab-bd0d-173718d99512",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_event_mask = nCluster != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5349966d-a151-4996-87bc-8ba2ef861cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_event = all_events[filtered_event_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "73ccceb0-76e5-4886-b091-37249245bd0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* selected events:  13403 / 20000\n"
     ]
    }
   ],
   "source": [
    "print(\"* selected events: \", len(filtered_event), \"/\", len(all_events))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58e011f-a3cc-419e-a320-fc8917a42b89",
   "metadata": {},
   "source": [
    "First event does not have clusters and was filtered out:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12f9584-0239-4522-b12a-3447771a20a2",
   "metadata": {},
   "source": [
    "Second event have one cluster with 105 cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f37665-8f25-4871-8a64-d6a6d062801b",
   "metadata": {},
   "source": [
    "# Loop over events to create an index array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0f2ca60d-0cde-44dd-acb5-35945a2bf9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================================#\n",
    "## CREATE INDEX ARRAY FOR  CLUSTERS ##\n",
    "#============================================#\n",
    "event_indices = []\n",
    "t0 = t.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4421d837-6483-4a0e-81d1-b6a29c24e948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Event  3000  has  1 / 2  selected clusters\n",
      "** Event  5000  has  3 / 4  selected clusters\n",
      "** Event  11000  has  3 / 3  selected clusters\n",
      "** Event  13000  has  1 / 1  selected clusters\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    for evt in filtered_event:\n",
    "        # pull cluster number, don't need zero index as it's loaded as a np array\n",
    "        nClust = event_dict[\"nCluster\"][evt]\n",
    "        cluster_idx = np.arange(nClust)\n",
    "\n",
    "        ## Cluster properties\n",
    "        clusterEs = event_dict[\"cluster_E\"][evt].to_numpy()\n",
    "        clus_phi = event_dict[\"cluster_Phi\"][evt].to_numpy()\n",
    "        clus_eta = event_dict[\"cluster_Eta\"][evt].to_numpy()\n",
    "\n",
    "        ## ENERGY SELECTION AND ETA SELECTION\n",
    "        eta_mask = abs(clus_eta) < 0.7\n",
    "        e_mask = clusterEs > 0.5\n",
    "        selection = eta_mask & e_mask\n",
    "        selected_clusters = cluster_idx[selection]\n",
    "\n",
    "        ## CREATE LIST ##\n",
    "        if np.count_nonzero(selection) > 0:\n",
    "            event_indices.append((evt, 0, selected_clusters))\n",
    "            if (evt % 1000==0):\n",
    "                print(\"** Event \", evt, \" has \", np.count_nonzero(selection), \"/\" ,nClust,\" selected clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "710ab897-3149-4a64-b467-f3347ecc67ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_indices = np.array(event_indices, dtype=np.object_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3af606-bcee-474b-88bd-8c812d58f440",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "672d1147-a029-4168-93c9-1a6d6ccfcb15",
   "metadata": {},
   "source": [
    "# Max dimension of X array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5ddb282d-93a3-4733-9d6a-5f6a1bf3a9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Events with selected clusters: 3722\n",
      "* Total number of cells: 942\n",
      "* Dim of largest point cloud: 942\n"
     ]
    }
   ],
   "source": [
    "    #=========================#\n",
    "    ## DIMENSIONS OF X ARRAY ##\n",
    "    #=========================#\n",
    "    max_dims = find_max_dim_tuple(event_indices, event_dict)\n",
    "    evt_tot = max_dims[0]\n",
    "    tot_nEvts += max_dims[0]\n",
    "    # keep track of the largest point cloud to use for saving later\n",
    "    if max_dims[1] > max_nPoints:\n",
    "        max_nPoints = max_dims[1]\n",
    "\n",
    "    print('* Events with selected clusters: '+str(evt_tot))\n",
    "    print('* Total number of cells: '+str(max_dims[1]))\n",
    "    print('* Dim of largest point cloud: '+str(max_nPoints))\n",
    "\n",
    "    # Create arrays\n",
    "    Y_new = np.zeros((max_dims[0],3))\n",
    "    X_new = np.zeros(max_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1823431-39f8-4ab7-82e6-6c6db4171bdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e617af4-cd58-4a1a-8e6b-f59e64ddcd15",
   "metadata": {},
   "source": [
    "# Fill the entires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d035c647-b45c-4394-b1a0-eda9dddc49d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(max_dims[0]):\n",
    "        # pull all relevant indices\n",
    "        evt = event_indices[i,0]\n",
    "        track_idx = event_indices[i,1]\n",
    "        # recall this now returns an array\n",
    "        cluster_nums = event_indices[i,2]\n",
    "\n",
    "        ##############\n",
    "        ## CLUSTERS ##\n",
    "        ##############\n",
    "        # set up to have no clusters, further this with setting up the same thing for tracks\n",
    "        target_ENG_CALIB_TOT = -1\n",
    "        if cluster_nums is not None:\n",
    "\n",
    "            # find averaged center of clusters\n",
    "            cluster_Eta = event_dict['cluster_Eta'][evt].to_numpy()\n",
    "            cluster_Phi = event_dict['cluster_Phi'][evt].to_numpy()\n",
    "            av_Eta = np.mean(cluster_Eta)\n",
    "            av_Phi = np.mean(cluster_Phi)\n",
    "\n",
    "            nClust_current_total = 0\n",
    "            target_ENG_CALIB_TOT = 0\n",
    "\n",
    "            for c in cluster_nums:            \n",
    "                # cluster data\n",
    "                target_ENG_CALIB_TOT += event_dict['cluster_ENG_CALIB_TOT'][evt][c]\n",
    "                cluster_cell_ID = event_dict['cluster_cell_ID'][evt][c].to_numpy()\n",
    "                nInClust = len(cluster_cell_ID)\n",
    "                cluster_cell_E = event_dict['cluster_cell_E'][evt][c].to_numpy()            \n",
    "                cell_indices = find_index_1D(cluster_cell_ID, cell_ID_dict)\n",
    "\n",
    "                cluster_cell_Eta = geo_dict['cell_geo_eta'][cell_indices]\n",
    "                cluster_cell_Phi = geo_dict['cell_geo_phi'][cell_indices]\n",
    "                cluster_cell_rPerp = geo_dict['cell_geo_rPerp'][cell_indices]\n",
    "                cluster_cell_sampling = geo_dict['cell_geo_sampling'][cell_indices]\n",
    "\n",
    "                # input all the data\n",
    "                # note here we leave the fourth entry zeros (zero for flag!!!)\n",
    "                low = nClust_current_total\n",
    "                high = low + nInClust\n",
    "                X_new[i,low:high,0] = np.log(cluster_cell_E)\n",
    "                # Normalize to average cluster centers\n",
    "                X_new[i,low:high,1] = cluster_cell_Eta - av_Eta #cluster_cell_Eta - event_dict['cluster_Eta'][evt][c]\n",
    "                X_new[i,low:high,2] = cluster_cell_Phi - av_Phi #cluster_cell_Phi -event_dict['cluster_Phi'][evt][c]\n",
    "                X_new[i,low:high,3] = cluster_cell_rPerp\n",
    "                X_new[i,low:high,5] = cluster_cell_sampling * 0.1\n",
    "\n",
    "                nClust_current_total += nInClust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d4c3bce4-46ab-4b18-a42b-9e5049b00805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3722, 942, 6)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7db117f0-e92a-4c02-baec-bd6757b3f1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "## TARGET ENERGIES ##\n",
    "#####################\n",
    "# this is needed for energy regression\n",
    "Y_new[i,0] = event_dict['truthPartE'][evt][0]\n",
    "Y_new[i,1] = event_dict['truthPartPt'][evt][track_idx]\n",
    "Y_new[i,2] = target_ENG_CALIB_TOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7415f68f-1991-48f3-a7d3-66592eb9f304",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffc3259-4cf8-42de-a373-cec82f804b3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c232a2e-c7a3-4795-bb84-c6a036c9dbe2",
   "metadata": {},
   "source": [
    "# Create TFRecord"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8098b535-f09e-4fe0-9dbf-c5b7df371026",
   "metadata": {},
   "source": [
    "https://medium.com/mostly-ai/tensorflow-records-what-they-are-and-how-to-use-them-c46bc4bbb564"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d484648-3c54-482b-a0b7-1686dc61b1d6",
   "metadata": {},
   "source": [
    "Use tf.train.Example\n",
    "\n",
    "If your dataset consist of features, where each feature is a list of values of the same type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8978e657-76f1-4d00-8f93-97ea929d75d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-26 08:34:03.065275: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f76941b-a624-4c60-88ba-59ec6d4345e5",
   "metadata": {},
   "source": [
    "## Features\n",
    "\n",
    "Order in X_new[:,:,i]: i= 0:energy, 1:eta, 2:phi, 3:rperp, 4:sampleID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1c7292-db93-466b-8deb-63ec222fa372",
   "metadata": {
    "tags": []
   },
   "source": [
    "- *tf.train.Feature* wraps a list of data of a specific type so Tensorflow can understand it. \n",
    "\n",
    "\n",
    "single attribute: union of bytes_list/float_list/int64_list.\n",
    "the stored list can be of type tf.train.BytesList, tf.train.FloatList, or tf.train.Int64List "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "58762e94-aa77-4e50-98a2-bba4e6006c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _floats_feature(value):\n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7463fbf-59c0-4f89-a429-e1c05c0b831d",
   "metadata": {},
   "source": [
    "Features: Energy, $\\eta$, $\\phi$, sampleID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "009db457-8d6f-48d2-a3a6-8b8b1b125f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "ene_arr_flat= ak.flatten(X_new[:,:,0].astype(np.float64))\n",
    "eta_arr_flat= ak.flatten(X_new[:,:,1].astype(np.float64))\n",
    "phi_arr_flat= ak.flatten(X_new[:,:,2].astype(np.float64))\n",
    "sam_arr_flat= ak.flatten(X_new[:,:,4].astype(np.float64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530848f2-1edf-44bd-846e-2ba5498087d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "- *tf.train.Features* is a collection of named features.\n",
    "\n",
    "Single attribute: feature that expects a dictionary where the key is the name of the features and the value a tf.train.Feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "821ab41e-9878-495a-86dd-a3fbf0ea1b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dic = {\n",
    "    \"ene\": _floats_feature(ene_arr_flat),\n",
    "    \"eta\": _floats_feature(eta_arr_flat),\n",
    "    \"phi\": _floats_feature(phi_arr_flat),\n",
    "    \"sam\": _floats_feature(sam_arr_flat),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b97b8247-20ff-4d52-bee9-12743855a67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classif_features = tf.train.Features(feature=features_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2fc01b-7e92-4441-9382-e69c44913037",
   "metadata": {
    "tags": []
   },
   "source": [
    "- *tf.train.Example* stores features in a single attribute features of type tf.train.Features.\n",
    "\n",
    "\n",
    "Note: \n",
    "- *tf.train.SequenceExample* In contrast to tf.train.Example, it does not store a list of bytes, floats or int64s, but a *list of lists* of bytes, floats or int64s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ed902ae7-d137-489e-9bdd-f24685ba5950",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = tf.train.Example(features=classif_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "309b7bd4-3a60-4cbf-bfd0-e407ae1c79b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Just for eta\n",
    "example_eta = tf.train.Example(features=tf.train.Features(feature={\"eta\": _floats_feature(eta_arr_flat)}))\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6022f25a-37c2-49b5-a2c6-389dcbcf8e00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6174e618-fb1f-4832-8a0b-8c7de86eb64f",
   "metadata": {},
   "source": [
    "## Write"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933e2e8e-f1ca-40f5-b862-bf0e3d52c418",
   "metadata": {
    "tags": []
   },
   "source": [
    "- *tf.python_io.TFRecordWriter* to write\n",
    "\n",
    "Serialize and store the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a5ec39-8a8c-41d1-8d1e-4939d04a6a74",
   "metadata": {},
   "source": [
    "For tf2: tf.compat.v1.python_io.TFRecordWriter instead of tf.python_io.TFRecordWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "09fb0aae-de63-40eb-a1af-e97039dec790",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.compat.v1.python_io.TFRecordWriter('/data/atlas/dportill/X_1file.tfrecord') as writer:\n",
    "  writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a446958b-8358-47a2-8b79-4995678fd0e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95877a20-1c38-4049-88af-84b7ca7f2c20",
   "metadata": {},
   "source": [
    "Let's check the serialized example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1615b7be-95f7-4deb-b22e-4eb515eea18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "serialized_example = example.SerializeToString()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c687ff-564b-435d-8eef-27b61eeab303",
   "metadata": {},
   "source": [
    "To decode the message use the tf.train.Example.FromString method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2c4d29bd-76b1-4571-85b5-b8f1ebfa2f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_proto = tf.train.Example.FromString(serialized_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08483515-0f1b-4ee0-a4f8-544834c2dd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_proto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f001542-d9ed-4c3f-abd0-dbed969000af",
   "metadata": {},
   "source": [
    "## Read"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b1179f-09cf-46a9-9e5f-dc19947bc6a5",
   "metadata": {},
   "source": [
    "Initialize TFRecordDataset for the TFRecord file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7b079489-a88b-49fa-bb6f-cac123cdf9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initilizaing the TFRecordDataset for train and test TFRecord file\n",
    "tfrecord_dataset=tf.data.TFRecordDataset('/data/atlas/dportill/X_1file.tfrecord')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dbda89-09f4-4a85-9dfd-ade74009b97a",
   "metadata": {},
   "source": [
    "1) Create the feature dictionary describing the features using tf.FixedLenFeature and tf.VarLenFeature. This should match with the feature names used while writing the data to the TFRecord file.\n",
    "\n",
    "2) Extract the dictionary object using parse_single_example for each of the data records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d9a82277-6339-4b04-aa99-e4b6b3d2b9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tfrecord(serialized_example):\n",
    "    feature_description={\n",
    "        'ene':  tf.io.FixedLenFeature((), tf.float32),\n",
    "        'eta':  tf.io.FixedLenFeature((), tf.float32),\n",
    "        'phi':  tf.io.FixedLenFeature((), tf.float32),\n",
    "        'sam':  tf.io.FixedLenFeature((), tf.float32),               \n",
    "    }\n",
    "    example= tf.io.parse_single_example(serialized_example, feature_description)\n",
    "    \n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "8fa3c77c-0433-41f1-b6bc-96d919136671",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tfrecord_dataset.map(read_tfrecord)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd9644b-8a29-4250-9acf-54eb77e5e7c2",
   "metadata": {},
   "source": [
    "## Create an input pipeline\n",
    "\n",
    "Prefetch, shuffle create a batch of records from the training dataset but skip shuffling for the test dataset.\n",
    "(haven't dive it yet into train/test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "e0b4d1a9-b4ad-48c9-b2da-4c8ccad80bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "dataset = dataset.shuffle(True)\n",
    "dataset = dataset.batch(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f069b39a-88d0-4c62-8301-279339392bf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbdc1c48-9f45-461f-8146-cbdc69a7b425",
   "metadata": {},
   "source": [
    "# Initialise a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393f09c8-9dcf-4d15-9c1c-8507feeb14df",
   "metadata": {},
   "source": [
    "\n",
    "it seems like it might be possible to avoid the TFRecords and use the numpy files directly via tf.data.Dataset.from_generator ? That would let us use the Dataset formalism (and the shuffling etc. it allows) without worrying about TFRecords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c5fc02-ad8d-4263-8c59-21104a16fdfd",
   "metadata": {},
   "source": [
    "tf.data.Dataset.from_generator ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cc03e4-c791-4857-bc55-751901ebccea",
   "metadata": {},
   "source": [
    "* Importing data from numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d731692b-9cee-4b42-98fc-44e0e3351dc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_X_new = tf.data.Dataset.from_tensor_slices(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19159ac3-1e73-42fa-8e11-c0fc52da915e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_X_new' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36196/2047315607.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset_X_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset_X_new' is not defined"
     ]
    }
   ],
   "source": [
    "dataset_X_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493238a0-5cda-49a9-82dd-054ff25ccf4a",
   "metadata": {},
   "source": [
    "* Importing data from generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "497b86f9-ba5c-4f6b-83e0-3f880c72c1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "673eefc7-fe6c-4d7d-a104-7917c8632e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator():\n",
    "    for el in sequence:\n",
    "        yield el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "2da3472b-5c39-44e4-aa9c-96239cad6288",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't instantiate abstract class DatasetV2 with abstract methods _inputs, element_spec",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_50842/3443176163.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: Can't instantiate abstract class DatasetV2 with abstract methods _inputs, element_spec"
     ]
    }
   ],
   "source": [
    "dataset_generator = tf.data.Dataset().batch(1).from_generator(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c7b1ee-9b27-4adb-a234-31b7fe41161f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86911113-86d6-4a82-a31c-4f9ac5407667",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac7ab2e-87b1-4f12-b4d4-a97b785f3644",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97179df1-3475-407b-9548-2de84ae9036a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b74ecc1-7c13-4009-9301-cad2ad6f3f04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "276bc7d4-d3b1-4bb9-8e22-854708cbef88",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Read and print data:\\nsess = tf.InteractiveSession()\\n\\n# Read TFRecord file\\nreader = tf.TFRecordReader()\\nfilename_queue = tf.train.string_input_producer(['/data/atlas/dportill/X_1file.tfrecord'])\\n\\n_, serialized_example = reader.read(filename_queue)\\n\\n# Define features\\n\\nsequence_features = {\\n    'ene': tf.FixedLenSequenceFeature([], dtype=tf.float32),\\n}\\n\\n# Extract features from serialized data\\nsequence_data = tf.parse_single_sequence_example(\\n    serialized=serialized_example, sequence_features=sequence_features)\\n\\n# Many tf.train functions use tf.train.QueueRunner,\\n# so we need to start it before we read\\ntf.train.start_queue_runners(sess)\\n\\n\\nprint('\\nData')\\nfor name, tensor in sequence_data.items():\\n    print('{}: {}'.format(name, tensor.eval()))\\n    \\n\""
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Before TensorFlow 2:\n",
    "\"\"\"\n",
    "# Read and print data:\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Read TFRecord file\n",
    "reader = tf.TFRecordReader()\n",
    "filename_queue = tf.train.string_input_producer(['/data/atlas/dportill/X_1file.tfrecord'])\n",
    "\n",
    "_, serialized_example = reader.read(filename_queue)\n",
    "\n",
    "# Define features\n",
    "\n",
    "sequence_features = {\n",
    "    'ene': tf.FixedLenSequenceFeature([], dtype=tf.float32),\n",
    "}\n",
    "\n",
    "# Extract features from serialized data\n",
    "sequence_data = tf.parse_single_sequence_example(\n",
    "    serialized=serialized_example, sequence_features=sequence_features)\n",
    "\n",
    "# Many tf.train functions use tf.train.QueueRunner,\n",
    "# so we need to start it before we read\n",
    "tf.train.start_queue_runners(sess)\n",
    "\n",
    "\n",
    "print('\\nData')\n",
    "for name, tensor in sequence_data.items():\n",
    "    print('{}: {}'.format(name, tensor.eval()))\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3766286d-bd34-493b-b08b-500f2c4643c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5dc4c5-6098-42ca-9bed-28a6b1b65c62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b72a4b-d5b4-4c37-b69f-a35a437f1554",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039a4528-cafc-48f6-9daf-bcaa69fc6f9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9160c405-f04f-4f31-b450-cc1081618be5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8475c4a-1f81-4f54-8a96-43f59f2e44c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe051b4-f47c-4a84-ab1f-77380fbd34ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
